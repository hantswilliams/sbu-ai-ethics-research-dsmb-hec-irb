# Introduction

The emergence of powerful generative artificial intelligence (AI) models, particularly large language models (LLMs) such as OpenAI's GPT-4.1, Google's Gemini 1.5 Pro, Anthropic's Claude 3 Opus, and X.AI's Grok-1, has sparked significant interest in their potential applications across healthcare domains. These models demonstrate remarkable capabilities in natural language understanding, reasoning, and knowledge synthesis that could augment human decision-making in complex scenarios. Of particular interest is their potential role in supporting ethical decision-making processes within healthcare institutions—specifically within ethics oversight bodies such as Institutional Review Boards (IRBs), Hospital Ethics Committees (HECs), and Data Safety Monitoring Boards (DSMBs). Healthcare ethical decisions frequently involve complex considerations balancing multiple competing principles: patient autonomy, beneficence, non-maleficence, and justice. These decisions often occur under time constraints, with incomplete information, and amid evolving clinical circumstances. Traditional approaches to healthcare ethics deliberation, while thorough, can be resource-intensive, inconsistent across institutions, and subject to individual biases and limitations in human cognitive processing. The integration of generative AI into healthcare ethics frameworks presents a novel opportunity to enhance—not replace—human ethical deliberation through rapid analysis, systematic application of ethical principles, identification of potential blind spots, and standardization of ethical reasoning processes. However, before such integration can be responsibly implemented, rigorous evaluation of these models' capabilities, limitations, and alignment with established ethical frameworks is essential.

Early research examining the potential of generative AI in healthcare ethics has revealed both promise and significant limitations. Jenkins et al. (2024) conducted a pilot study evaluating ChatGPT's ability to write clinical ethics consultation notes, finding that while baseline performance was poor, the quality improved significantly when the model was provided with examples of past ethics consults, suggesting that with proper guidance, AI can approach acceptable levels of ethical analysis. Similarly, Rashid et al. (2024) assessed ChatGPT's "moral competence" in healthcare ethics dilemmas using Kohlberg's stages of moral reasoning, concluding that while GPT-4 demonstrated higher moral reasoning consistency than earlier versions, it still exhibited only "medium" moral competence when tackling complex healthcare ethics scenarios. In the domain of research ethics, Fukataki et al. (2024) found that GPT-4 could reliably extract key information from clinical trial protocols and consent forms with high accuracy (80-100% on certain elements), suggesting potential applications in supporting ethics committee evaluations. Despite these advances, Benzinger et al. (2023) emphasize in their systematic review that current ethical AI tools still face significant challenges, including lack of true empathy, risk of algorithmic bias, and the fundamental difficulty of encoding moral reasoning into computational frameworks.

Despite growing interest in AI applications for healthcare ethics, systematic evaluations of generative AI models in this domain remain limited. Several knowledge gaps exist: First, most evaluations of generative AI focus on general reasoning or simplified ethical dilemmas rather than the complex, nuanced scenarios encountered in clinical practice. Second, few studies directly compare multiple leading generative AI models using standardized prompts and evaluation criteria specific to healthcare ethics. Third, limited research exists on structured methodologies for effectively integrating AI into existing ethics oversight bodies like IRBs, HECs, and DSMBs. Finally, the concept of continuous, AI-driven ethics and safety monitoring (what we term a Data Safety Monitoring Agent or DSMA) remains largely unexplored. These gaps are particularly significant in high-impact clinical scenarios such as organ transplantation and oncology, where complex ethical decisions about resource allocation, treatment aggressiveness, and quality of life considerations are routine. For instance, Drezga-Kleiminger et al. (2023) surveyed public attitudes toward AI use in liver transplant allocation, finding that while 69% of respondents found AI-based allocation acceptable, concerns about "dehumanization" of sensitive decisions remained. Similarly, a 2024 survey of oncologists by Hantel et al. revealed that 81% felt patients should provide informed consent before AI is used in treatment decisions, highlighting the ethical complexity of integrating AI into critical clinical domains.

This study aims to address these knowledge gaps through a systematic evaluation of generative AI models in healthcare ethics contexts. Specifically, we seek to: 1) Evaluate the ethical reasoning capabilities of leading generative AI models using real-world clinical ethics scenarios derived from documented clinical ethics cases; 2) Analyze the concordance between AI-generated ethical recommendations and established ethical principles; 3) Assess response consistency, ethical principle alignment, and reasoning transparency across multiple models; 4) Develop preliminary guidelines for the potential integration of generative AI as a supportive tool within healthcare ethics committees; and 5) Explore the conceptual framework for a Data Safety Monitoring Agent (DSMA)—an AI-driven system for continuous ethics and safety monitoring in clinical trials and patient care.

This research has potential implications for multiple stakeholders in healthcare ethics: Ethics committees and IRBs may benefit from findings that inform how generative AI could augment deliberative processes, improve consistency, and enhance the thoroughness of ethical analyses. Clinical researchers could gain insights into novel approaches for continuous ethics monitoring in clinical trials through the DSMA concept. Healthcare institutions may utilize evidence-based guidelines to support responsible implementation of AI ethics tools in organizational workflows. AI developers might leverage identified strengths and limitations to guide future refinements of generative AI models for healthcare ethics applications. Regulatory bodies may incorporate findings to inform policy development regarding the appropriate role of AI in formal ethics oversight processes. By systematically evaluating generative AI in healthcare ethics contexts, this study contributes to the responsible advancement of AI as a supportive tool that enhances—rather than replaces—human ethical judgment in healthcare decision-making, potentially addressing what some researchers have called "cognitive moral enhancement" for clinicians by prompting more systematic ethical analysis while maintaining the essential human dimensions of empathy and contextual understanding.