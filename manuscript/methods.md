# Methods

## Study Design Overview

This study employed a systematic evaluation approach to assess the capabilities of generative AI models in healthcare ethics decision-making contexts. We utilized a standardized prompt-based testing methodology applied to real-world clinical ethics scenarios, with subsequent analysis of AI-generated recommendations compared to documented human ethics committee decisions. Our research design integrated both computational analysis of AI-generated content and human evaluation by health informatics graduate students to provide a comprehensive assessment of AI ethical reasoning capabilities. The methodology consisted of five main components: (1) selection of diverse healthcare ethics scenarios from documented clinical cases, (2) development of a structured prompt framework to elicit ethical analyses from AI models, (3) generation of responses using four leading generative AI models, (4) comprehensive evaluation of responses by Masters of Applied Health Informatics students using the SummEval framework, and (5) quantitative analysis of evaluation scores. This approach allowed us to assess the current capabilities and limitations of AI in healthcare ethics contexts while identifying differences in performance across model architectures and scenario types.

## Ethics Scenario Selection

### Data Source
We selected five real-world clinical ethics case studies from publicly documented cases at Brigham and Women's Hospital, available through their Clinical Ethics Case Review repository (https://bwhclinicalandresearchnews.org/clinical-ethics-case-review/). These cases were chosen to represent a diverse range of ethical dilemmas commonly encountered in healthcare settings. The selection process prioritized scenarios that would challenge AI systems with complex ethical reasoning requirements while representing issues that healthcare ethics committees regularly encounter in practice.

### Selected Scenarios
The five scenarios represented diverse clinical contexts, patient demographics, and ethical challenges: (1) A young international lymphoma patient with cross-cultural end-of-life conflict regarding extubation, where cultural factors influenced decision-making and created tension between surrogate decision-making and perceived medical futility; (2) A middle-aged adult with mental health disorders repeatedly presenting to the Emergency Department with intentional foreign body ingestion, highlighting ethical challenges related to self-harm behaviors, procedural futility, and appropriate resource allocation; (3) A middle-aged adult with infectious endocarditis related to injection drug use, requiring long-term antibiotics via PICC line, with ethical tension centered on harm-reduction approaches versus paternalism in discharge planning for patients with active addiction; (4) An older adult with catastrophic stroke, where family members requested posthumous sperm retrieval, exploring complex issues around posthumous reproduction and cultural inheritance pressures; and (5) An adult with terminal cancer whose surrogate decision-maker refused adequate pain management, focusing on the ethical dilemma of surrogate refusal of pain relief and the tension between palliative care obligations versus proxy authority.

These scenarios spanned multiple clinical departments, including ICU, Oncology, Emergency Medicine, Cardiology, Psychiatry, Neurology, and Palliative Care, reflecting the diverse contexts in which ethical dilemmas arise. The cases also represented multiple ICD-10 diagnostic categories, including neoplasms, mental and behavioral disorders, and diseases of the circulatory system.

### Selection Criteria
Cases were selected based on: complexity of ethical considerations and presence of clear ethical tensions; clear documentation of the clinical scenario and contextual factors; availability of the actual ethics committee decision or recommendation; relevance to current healthcare ethics practices; representation of diverse ethical principles (autonomy, beneficence, non-maleficence, justice); diversity in patient demographics, clinical contexts, and healthcare departments; and potential to challenge AI systems with nuanced ethical reasoning requirements.

## Generative AI Models

The study evaluated four leading generative AI models representing different commercial vendors and model architectures: (1) OpenAI GPT-4.1, an advanced large language model with enhanced reasoning capabilities, accessed through OpenAI API with temperature setting of 0.7; (2) Google Gemini 1.5 Pro, Google's advanced reasoning model designed for complex analytical tasks, accessed through Google AI Studio API with temperature setting of 0.7; (3) Anthropic Claude 3 Opus (claude-3-opus-20240229), Anthropic's large context window model known for detailed analytical capabilities, accessed through Anthropic API with temperature setting of 0.7; and (4) X.AI Grok-1, X.AI's conversational model with instruction-following capabilities, accessed through X.AI API with temperature setting of 0.7. These models were selected based on their widespread adoption in healthcare and research contexts, advanced reasoning capabilities, different architectural approaches, and representation of major AI development companies. The consistent temperature setting across models (0.7) enabled fair comparison of reasoning patterns while still allowing for natural language generation variability.

## Prompt Engineering

We developed a standardized prompt framework to provide clear guidance to the AI models while enabling meaningful comparison across responses. The prompt was designed to simulate the context of an ethics committee consultation while providing sufficient structure for systematic analysis. Our prompt framework positioned the AI as an Ethics Advisor embedded within a Hospital Ethics Committee at an academic medical center. The prompt established: (1) Role and Identity: Defined the AI as an advisor providing ethically rigorous, data-informed analyses to support human ethics committee members; (2) Ethical Principles: Explicitly referenced the core bioethical principles of autonomy, beneficence, non-maleficence, and justice as the foundation for analysis; (3) Scope and Limitations: Clarified that the AI should provide recommendations while acknowledging scenarios requiring uniquely human judgment; and (4) Clinical Domains: Outlined typical ethics committee scenarios including end-of-life care, transplantation, pediatric dilemmas, oncology decisions, and technology integration.

The standardized prompt instructed the AI to provide responses in a consistent format: a brief clinical scenario restatement; identification and explanation of relevant ethical principles and tensions; systematic ethical analysis; and explicitly stated recommended clinical decisions with a ranked list of medical recommendations (Recommended Decision as Best Medical Option, Alternative Decision as Second-Best Medical Option, and Least-Recommended Decision as Third Medical Option). The complete prompt template (prompt_v1.md) is available in the project repository.

## Data Collection Procedure

Each ethics scenario was processed through all four AI models (GPT-4.1, Gemini 1.5 Pro, Claude 3 Opus, and Grok-1) using the standardized prompt. We generated a single high-quality response for each scenario-model combination, resulting in 20 distinct AI-generated ethics committee responses (5 scenarios Ã— 4 models). All testing was conducted between May-June 2025, with prompts submitted through the respective model APIs using consistent parameter settings (temperature=0.7, max_tokens=4000) to ensure reproducibility while allowing for natural language generation.

AI-generated responses were systematically captured in their entirety, preserving all content, formatting, and metadata. Responses were stored in a structured SQLite database that included: AI Model identifier and vendor (OpenAI, Google, Anthropic, X.AI); scenario identifier (case number 1-5); complete timestamp of generation; processing time (milliseconds); complete response text; and extracted recommended decisions (primary, secondary, tertiary options). For analysis purposes, responses were processed to extract key components such as recommended courses of action, ethical principles referenced, and reasoning patterns.

## Evaluation Framework

### Human Evaluation Using SummEval Framework
To systematically assess the quality of AI-generated ethical analyses and recommendations, we implemented a comprehensive human evaluation process based on the SummEval framework (Fabbri et al., TACL 2021). This widely-used four-way rubric provides a structured approach for evaluating AI-generated content across key dimensions. We recruited 42 graduate students from the Masters of Applied Health Informatics program at Stony Brook University to serve as evaluators. These evaluators represented a population with foundational knowledge in healthcare systems, informatics principles, and basic exposure to healthcare ethics.

Each evaluator was assigned responses from all four AI models for a single case scenario. This design allowed for direct comparison of model performance on identical clinical ethics dilemmas while controlling for evaluator effects. For each of the five scenarios, approximately 8-9 evaluators independently assessed model responses. The evaluation used a single-blind methodology where evaluators were unaware which model generated each response they reviewed. Each evaluator completed a structured questionnaire with quantitative ratings for each response. This approach focused exclusively on capturing quantitative assessments rather than qualitative feedback.

Following the SummEval framework, evaluators assessed four key dimensions using a 5-point Likert scale (1=poor, 5=excellent): (1) Relevance: The degree to which the AI response addresses the core ethical issues in the scenario, evaluating identification of pertinent ethical considerations and assessing appropriateness of recommendations to the specific scenario; (2) Consistency/Correctness: The factual accuracy and ethical soundness of the analysis, evaluating alignment with established bioethical principles and assessing internal consistency of ethical reasoning; (3) Fluency: The linguistic quality and clarity of the AI-generated text, evaluating grammatical correctness and appropriate terminology, and assessing readability and professional tone; and (4) Coherence: The logical organization and flow of the ethical analysis, evaluating logical progression of ethical reasoning and assessing clear connections between principles and recommendations.

## Data Analysis

Our analysis focused on quantitative evaluation metrics and computational response characteristics. For the human evaluations, we conducted descriptive statistical analysis for each SummEval dimension (Relevance, Consistency/Correctness, Fluency, and Coherence), calculating mean scores, standard deviations, and confidence intervals for each model across all scenarios. Between-model comparisons were performed using analysis of variance (ANOVA) with post-hoc Tukey HSD tests to identify statistically significant differences in performance. To assess relationships between evaluation dimensions, we calculated Pearson correlation coefficients and conducted principal component analysis to identify underlying patterns in evaluator ratings.

For the AI-generated responses, we performed computational analyses including: ethical principle coverage (counting mentions of autonomy, beneficence, non-maleficence, and justice across models); response length analysis (comparing text length across models and scenarios); and processing time analysis (examining computational efficiency differences between models). We also analyzed vendor and model comparisons across scenarios to identify potential variations in performance based on ethical case types. Results were visualized through comparative charts displaying performance across models and scenarios, providing a comprehensive view of how different AI systems approach varied ethical challenges in healthcare.


