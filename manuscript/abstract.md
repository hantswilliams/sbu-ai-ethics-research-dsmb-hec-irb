# Abstract

This study evaluates the capabilities of four state-of-the-art generative AI models (Anthropic Claude 3 Opus, Google Gemini 1.5 Pro, OpenAI GPT-4.1, and X.AI Grok-1) in analyzing complex healthcare ethics scenarios. Using a standardized prompt, we tested these models on five real world clinical ethics committee cases involving cultural considerations, self-harm, addiction, reproduction, and end-of-life management. Performance was assessed through quantitative analysis of processing time, response length, ethical principle coverage, and recommendation consistency. A sample of n=44 graduate-level evaluators from a Masters in Applied Health Informatics program at Stony Brook University assessed the AI-generated responses using the SummEval framework (Fabbri et al., TACL 2021) dimensions of relevance, correctness/consistency, fluency, and coherence, resulting in 857 total evaluations. Results indicate all models demonstrated competence in analyzing clinical ethics scenarios, with overall scores ranging from 3.82 to 4.10 on a 5-point scale. While Claude 3 Opus provided the most comprehensive coverage of ethical principles and excelled in fluency and coherence, Grok-1 received the highest overall human evaluation scores, while generating the shortest responses. All models performed consistently on end-of-life and cultural scenarios compared to the variable performance observed on reproduction, addiction, and self-harm scenarios. These findings suggest generative AI holds promise as a supportive tool in clinical ethics committee work, though significant limitations remain regarding bias, transparency, and contextual understanding.
