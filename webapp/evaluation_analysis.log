2025-07-14 19:11:46,887 - evaluation_analysis - INFO - Using database at /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/results.db
2025-07-14 19:11:46,891 - evaluation_analysis - INFO - Loaded 18 responses from database
2025-07-14 19:11:46,892 - evaluation_analysis - INFO - Loaded 20 evaluations from database
2025-07-14 19:11:46,895 - evaluation_analysis - INFO - Loaded 20 evaluations from database
2025-07-14 19:11:46,901 - evaluation_analysis - INFO - Evaluations by evaluator:
evaluator_id
1    18
2     1
3     1
dtype: int64
2025-07-14 19:11:46,902 - evaluation_analysis - INFO - Evaluations by scenario:
case_id  scenario_filename  iteration
1        1_case.md          1            2
                            2            3
                            3            3
2        2_case.md          1            2
                            2            2
                            3            2
3        3_case.md          1            2
                            2            2
                            3            2
dtype: int64
2025-07-14 19:11:46,913 - evaluation_analysis - INFO - Average scores by vendor:
       relevance_score                  ... coherence_score                
                  mean       std count  ...            mean       std count
vendor                                  ...                                
Google        3.777778  1.394433     9  ...        3.444444  1.013794     9
OpenAI        3.363636  0.674200    11  ...        3.363636  0.809040    11

[2 rows x 12 columns]
2025-07-14 19:11:46,915 - evaluation_analysis - INFO - Overall average by vendor:
            mean       std  count
vendor                           
Google  3.638889  1.139566      9
OpenAI  3.409091  0.691671     11
2025-07-14 19:11:47,407 - evaluation_analysis - INFO - Score distribution plots saved
2025-07-14 19:11:47,673 - evaluation_analysis - INFO - Vendor comparison plots saved
2025-07-14 19:11:47,885 - evaluation_analysis - INFO - Model comparison plots saved
2025-07-14 19:11:47,900 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-14 19:11:47,902 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-14 19:11:47,967 - evaluation_analysis - INFO - Comprehensive evaluation report saved to /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/evaluation_results/evaluation_report.html
2025-07-14 19:11:47,967 - evaluation_analysis - INFO - Evaluation analysis completed successfully
2025-07-15 09:15:16,776 - evaluation_analysis - INFO - Using database at /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/results.db
2025-07-15 09:15:16,794 - evaluation_analysis - INFO - Loaded 4 responses from database
2025-07-15 09:15:16,800 - evaluation_analysis - INFO - Loaded 4 evaluations from database
2025-07-15 09:15:16,810 - evaluation_analysis - INFO - Loaded 4 evaluations from database
2025-07-15 09:15:16,859 - evaluation_analysis - INFO - Evaluations by evaluator:
evaluator_id
1    4
dtype: int64
2025-07-15 09:15:16,861 - evaluation_analysis - INFO - Evaluations by scenario:
case_id  scenario_filename  iteration
1        1_case.md          1            4
dtype: int64
2025-07-15 09:15:16,891 - evaluation_analysis - INFO - Average scores by vendor:
          relevance_score           correctness_score  ... fluency_score coherence_score          
                     mean std count              mean  ...         count            mean std count
vendor                                                 ...                                        
Anthropic             2.0 NaN     1               3.0  ...             1             4.0 NaN     1
GROK                  3.0 NaN     1               3.0  ...             1             3.0 NaN     1
Google                5.0 NaN     1               4.0  ...             1             2.0 NaN     1
OpenAI                4.0 NaN     1               5.0  ...             1             4.0 NaN     1

[4 rows x 12 columns]
2025-07-15 09:15:16,901 - evaluation_analysis - INFO - Overall average by vendor:
           mean  std  count
vendor                     
Anthropic  3.25  NaN      1
GROK       3.00  NaN      1
Google     3.75  NaN      1
OpenAI     4.25  NaN      1
2025-07-15 09:15:18,101 - evaluation_analysis - INFO - Score distribution plots saved
2025-07-15 09:15:19,012 - evaluation_analysis - INFO - Vendor comparison plots saved
2025-07-15 09:15:19,913 - evaluation_analysis - INFO - Model comparison plots saved
2025-07-15 09:15:19,957 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-15 09:15:19,962 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-15 09:15:20,121 - evaluation_analysis - INFO - Comprehensive evaluation report saved to /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/evaluation_results/evaluation_report.html
2025-07-15 09:15:20,121 - evaluation_analysis - INFO - Evaluation analysis completed successfully
2025-07-15 09:27:55,811 - evaluation_analysis - INFO - Using database at /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/results.db
2025-07-15 09:27:55,815 - evaluation_analysis - INFO - Loaded 4 responses from database
2025-07-15 09:27:55,816 - evaluation_analysis - INFO - Loaded 8 evaluations from database
2025-07-15 09:27:55,823 - evaluation_analysis - INFO - Loaded 8 evaluations from database
2025-07-15 09:27:55,835 - evaluation_analysis - INFO - Evaluations by evaluator:
evaluator_id
1    4
2    4
dtype: int64
2025-07-15 09:27:55,835 - evaluation_analysis - INFO - Evaluations by scenario:
case_id  scenario_filename  iteration
1        1_case.md          1            8
dtype: int64
2025-07-15 09:27:55,845 - evaluation_analysis - INFO - Average scores by vendor:
          relevance_score                  ... coherence_score                
                     mean       std count  ...            mean       std count
vendor                                     ...                                
Anthropic             2.5  0.707107     2  ...             3.5  0.707107     2
GROK                  3.0  0.000000     2  ...             3.0  0.000000     2
Google                3.5  2.121320     2  ...             1.5  0.707107     2
OpenAI                4.0  0.000000     2  ...             4.0  0.000000     2

[4 rows x 12 columns]
2025-07-15 09:27:55,846 - evaluation_analysis - INFO - Overall average by vendor:
           mean       std  count
vendor                          
Anthropic  3.25  0.000000      2
GROK       3.00  0.000000      2
Google     3.00  1.060660      2
OpenAI     4.00  0.353553      2
2025-07-15 09:27:56,446 - evaluation_analysis - INFO - Score distribution plots saved
2025-07-15 09:27:56,944 - evaluation_analysis - INFO - Vendor comparison plots saved
2025-07-15 09:27:57,332 - evaluation_analysis - INFO - Model comparison plots saved
2025-07-15 09:27:57,396 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-15 09:27:57,398 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-15 09:27:57,475 - evaluation_analysis - INFO - Comprehensive evaluation report saved to /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/evaluation_results/evaluation_report.html
2025-07-15 09:27:57,475 - evaluation_analysis - INFO - Evaluation analysis completed successfully
2025-07-17 08:41:34,888 - ai_ethics.db - INFO - Using Supabase PostgreSQL database
2025-07-17 08:41:34,888 - evaluation_analysis - INFO - Using supabase database
2025-07-17 08:41:35,562 - evaluation_analysis - INFO - Loaded 20 responses from database
2025-07-17 08:41:35,906 - evaluation_analysis - INFO - Loaded 164 evaluations from database
2025-07-17 08:41:35,908 - evaluation_analysis - INFO - Loaded 164 evaluations from database
2025-07-17 08:41:35,914 - evaluation_analysis - INFO - Evaluations by evaluator:
evaluator_id
9      2
10     4
14    20
16     5
18    20
20     4
21    20
22    20
24     8
25     8
26     3
27    22
28    20
29     8
dtype: int64
2025-07-17 08:41:35,914 - evaluation_analysis - INFO - Evaluations by scenario:
case_id  scenario_filename  iteration
1        1_case.md          1            48
2        2_case.md          1            38
3        3_case.md          1            26
4        4_case.md          1            25
5        5_case.md          1            27
dtype: int64
2025-07-17 08:41:35,918 - evaluation_analysis - INFO - Average scores by vendor:
          relevance_score                 correctness_score                 fluency_score                 coherence_score                
                     mean       std count              mean       std count          mean       std count            mean       std count
vendor                                                                                                                                   
Anthropic        4.175000  0.747217    40          4.100000  0.841244    40      4.125000  0.852974    40        4.200000  0.822753    40
GROK             4.261905  0.828149    42          4.285714  0.741972    42      4.166667  0.695514    42        4.142857  0.813647    42
Google           4.047619  0.909365    42          3.857143  0.871540    42      4.000000  0.855399    42        3.880952  1.040694    42
OpenAI           4.075000  0.828576    40          4.000000  0.877058    40      4.025000  0.767530    40        4.000000  0.784465    40
2025-07-17 08:41:35,921 - evaluation_analysis - INFO - Overall average by vendor:
               mean       std  count
vendor                              
Anthropic  4.150000  0.690596     40
GROK       4.214286  0.684255     42
Google     3.946429  0.836543     42
OpenAI     4.025000  0.699817     40
2025-07-17 08:41:36,432 - evaluation_analysis - INFO - Score distribution plots saved
2025-07-17 08:41:36,755 - evaluation_analysis - INFO - Vendor comparison plots saved
2025-07-17 08:41:37,027 - evaluation_analysis - INFO - Model comparison plots saved
2025-07-17 08:41:37,040 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-17 08:41:37,042 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.
2025-07-17 08:41:37,089 - evaluation_analysis - INFO - Comprehensive evaluation report saved to /Users/hantswilliams/Development/python/sbu-ai-ethics-research-dsmb-hec-irb/data/evaluation_results/evaluation_report.html
2025-07-17 08:41:37,089 - evaluation_analysis - INFO - Evaluation analysis completed successfully
